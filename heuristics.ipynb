{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puzzles shape: (1200, 10, 10)\n",
      "Solutions shape: (1200, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load(\"star_battle_dataset.npz\", allow_pickle=True)\n",
    "puzzles = dataset[\"puzzles\"]\n",
    "solutions = dataset[\"solutions\"]\n",
    "# Check the shape of the dataset\n",
    "print(\"Puzzles shape:\", puzzles.shape)\n",
    "print(\"Solutions shape:\", solutions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original puzzle count: 1200\n",
      "Augmented puzzle count: 7200\n"
     ]
    }
   ],
   "source": [
    "# augment data for larger dataset\n",
    "def augment_data(puzzles, solutions):\n",
    "    augmented_puzzles = []\n",
    "    augmented_solutions = []\n",
    "    \n",
    "    for i in range(len(puzzles)):\n",
    "        puzzle = puzzles[i]\n",
    "        solution = solutions[i]\n",
    "        \n",
    "        # original puzzle\n",
    "        augmented_puzzles.append(puzzle)\n",
    "        augmented_solutions.append(solution)\n",
    "        \n",
    "        # horizontal Flip\n",
    "        augmented_puzzles.append(np.fliplr(puzzle))\n",
    "        augmented_solutions.append(np.fliplr(solution))\n",
    "        \n",
    "        # vertical Flip\n",
    "        augmented_puzzles.append(np.flipud(puzzle))\n",
    "        augmented_solutions.append(np.flipud(solution))\n",
    "        \n",
    "        # rotate 90 degrees\n",
    "        augmented_puzzles.append(np.rot90(puzzle, k=1))\n",
    "        augmented_solutions.append(np.rot90(solution, k=1))\n",
    "        \n",
    "        # rotate 180 degrees\n",
    "        augmented_puzzles.append(np.rot90(puzzle, k=2))\n",
    "        augmented_solutions.append(np.rot90(solution, k=2))\n",
    "        \n",
    "        # rotate 270 degrees\n",
    "        augmented_puzzles.append(np.rot90(puzzle, k=3))\n",
    "        augmented_solutions.append(np.rot90(solution, k=3))\n",
    "    \n",
    "    return np.array(augmented_puzzles), np.array(augmented_solutions)\n",
    "\n",
    "# apply augmentation to your dataset\n",
    "augmented_puzzles, augmented_solutions = augment_data(puzzles, solutions)\n",
    "\n",
    "print(\"Original puzzle count:\", len(puzzles))\n",
    "print(\"Augmented puzzle count:\", len(augmented_puzzles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (960, 10, 10, 1), (960, 10, 10, 1)\n",
      "Testing data shape: (240, 10, 10, 1), (240, 10, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape puzzles and solutions if necessary (ensure each puzzle is a 2D grid)\n",
    "augmented_puzzles = puzzles[..., np.newaxis] # Assuming 10x10 grids, change if needed\n",
    "augmented_solutions = solutions[..., np.newaxis]  # Same for solutions\n",
    "\n",
    "# Normalize puzzle values between 0 and 1 (if necessary)\n",
    "augmented_puzzles = puzzles.astype(np.float32) / 9.0\n",
    "\n",
    "augmented_solutions = solutions.astype(np.float32) \n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(augmented_puzzles, augmented_solutions, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_22 (Conv2D)          (None, 10, 10, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 10, 10, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPoolin  (None, 5, 5, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 5, 5, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 5, 5, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPoolin  (None, 2, 2, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 2, 2, 128)         8320      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 2, 2, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 100)               51300     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 10, 10, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 98,212\n",
      "Trainable params: 97,700\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(10, 10, 1)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(128, (1, 1), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(100, activation='sigmoid'),  # Predicts for all 100 cells\n",
    "    layers.Reshape((10, 10, 1))  # reshape back to 10x10 grid\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 3s 41ms/step - loss: 0.6321 - accuracy: 0.7105 - val_loss: 0.6663 - val_accuracy: 0.6349\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5165 - accuracy: 0.7885 - val_loss: 0.6531 - val_accuracy: 0.6305\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.4893 - accuracy: 0.7966 - val_loss: 0.6357 - val_accuracy: 0.6641\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4729 - accuracy: 0.8001 - val_loss: 0.6262 - val_accuracy: 0.6658\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4569 - accuracy: 0.8030 - val_loss: 0.6222 - val_accuracy: 0.6737\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4422 - accuracy: 0.8077 - val_loss: 0.6201 - val_accuracy: 0.6817\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.4264 - accuracy: 0.8136 - val_loss: 0.6219 - val_accuracy: 0.6806\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4096 - accuracy: 0.8209 - val_loss: 0.6246 - val_accuracy: 0.6832\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.3941 - accuracy: 0.8268 - val_loss: 0.6249 - val_accuracy: 0.6877\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3764 - accuracy: 0.8359 - val_loss: 0.6254 - val_accuracy: 0.6907\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.3573 - accuracy: 0.8454 - val_loss: 0.6306 - val_accuracy: 0.6930\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.3400 - accuracy: 0.8545 - val_loss: 0.6340 - val_accuracy: 0.6895\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.3202 - accuracy: 0.8654 - val_loss: 0.6263 - val_accuracy: 0.7015\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3016 - accuracy: 0.8753 - val_loss: 0.6278 - val_accuracy: 0.7021\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.2826 - accuracy: 0.8861 - val_loss: 0.6237 - val_accuracy: 0.7123\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.2651 - accuracy: 0.8956 - val_loss: 0.6214 - val_accuracy: 0.7165\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.2488 - accuracy: 0.9043 - val_loss: 0.6334 - val_accuracy: 0.7110\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.2315 - accuracy: 0.9142 - val_loss: 0.6352 - val_accuracy: 0.7195\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.2182 - accuracy: 0.9200 - val_loss: 0.6535 - val_accuracy: 0.7147\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.2021 - accuracy: 0.9289 - val_loss: 0.6561 - val_accuracy: 0.7250\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6561 - accuracy: 0.7250\n",
      "Test Accuracy: 0.7250\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy._core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstar_battle_dataset.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m puzzles \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpuzzles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m solutions \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolutions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Check the shape of the dataset\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\npyio.py:253\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\format.py:781\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    779\u001b[0m     pickle_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 781\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# Friendlier error message\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mUnicodeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnpickling a python object failed: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    785\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may need to pass the encoding= option \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    786\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto numpy.load\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (err,)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy._core'"
     ]
    }
   ],
   "source": [
    "dataset = np.load(\"star_battle_dataset.npz\", allow_pickle=True)\n",
    "puzzles = dataset[\"puzzles\"]\n",
    "solutions = dataset[\"solutions\"]\n",
    "# Check the shape of the dataset\n",
    "print(\"Puzzles shape:\", puzzles.shape)\n",
    "print(\"Solutions shape:\", solutions.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
