{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heuristic learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1200 dataset attempt cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puzzles shape: (1200, 10, 10)\n",
      "Solutions shape: (1200, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load(\"star_battle_dataset.npz\", allow_pickle=True)\n",
    "puzzles = dataset[\"puzzles\"]\n",
    "solutions = dataset[\"solutions\"]\n",
    "# Check the shape of the dataset\n",
    "print(\"Puzzles shape:\", puzzles.shape)\n",
    "print(\"Solutions shape:\", solutions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original puzzle count: 1200\n",
      "Augmented puzzle count: 7200\n"
     ]
    }
   ],
   "source": [
    "# augment data for larger dataset\n",
    "def augment_data(puzzles, solutions):\n",
    "    augmented_puzzles = []\n",
    "    augmented_solutions = []\n",
    "    \n",
    "    for i in range(len(puzzles)):\n",
    "        puzzle = puzzles[i]\n",
    "        solution = solutions[i]\n",
    "        \n",
    "        # original puzzle\n",
    "        augmented_puzzles.append(puzzle)\n",
    "        augmented_solutions.append(solution)\n",
    "        \n",
    "        # horizontal Flip\n",
    "        augmented_puzzles.append(np.fliplr(puzzle))\n",
    "        augmented_solutions.append(np.fliplr(solution))\n",
    "        \n",
    "        # vertical Flip\n",
    "        augmented_puzzles.append(np.flipud(puzzle))\n",
    "        augmented_solutions.append(np.flipud(solution))\n",
    "        \n",
    "        # rotate 90 degrees\n",
    "        augmented_puzzles.append(np.rot90(puzzle, k=1))\n",
    "        augmented_solutions.append(np.rot90(solution, k=1))\n",
    "        \n",
    "        # rotate 180 degrees\n",
    "        augmented_puzzles.append(np.rot90(puzzle, k=2))\n",
    "        augmented_solutions.append(np.rot90(solution, k=2))\n",
    "        \n",
    "        # rotate 270 degrees\n",
    "        augmented_puzzles.append(np.rot90(puzzle, k=3))\n",
    "        augmented_solutions.append(np.rot90(solution, k=3))\n",
    "    \n",
    "    return np.array(augmented_puzzles), np.array(augmented_solutions)\n",
    "\n",
    "# apply augmentation to your dataset\n",
    "augmented_puzzles, augmented_solutions = augment_data(puzzles, solutions)\n",
    "\n",
    "print(\"Original puzzle count:\", len(puzzles))\n",
    "print(\"Augmented puzzle count:\", len(augmented_puzzles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (960, 10, 10, 1), (960, 10, 10, 1)\n",
      "Testing data shape: (240, 10, 10, 1), (240, 10, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape puzzles and solutions if necessary (ensure each puzzle is a 2D grid)\n",
    "augmented_puzzles = puzzles[..., np.newaxis] # Assuming 10x10 grids, change if needed\n",
    "augmented_solutions = solutions[..., np.newaxis]  # Same for solutions\n",
    "\n",
    "# Normalize puzzle values between 0 and 1 (if necessary)\n",
    "augmented_puzzles = puzzles.astype(np.float32) / 9.0\n",
    "\n",
    "augmented_solutions = solutions.astype(np.float32) \n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(augmented_puzzles, augmented_solutions, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_22 (Conv2D)          (None, 10, 10, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 10, 10, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPoolin  (None, 5, 5, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 5, 5, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 5, 5, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPoolin  (None, 2, 2, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 2, 2, 128)         8320      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 2, 2, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 100)               51300     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 10, 10, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 98,212\n",
      "Trainable params: 97,700\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(10, 10, 1)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(128, (1, 1), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(100, activation='sigmoid'),  # Predicts for all 100 cells\n",
    "    layers.Reshape((10, 10, 1))  # reshape back to 10x10 grid\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 3s 41ms/step - loss: 0.6321 - accuracy: 0.7105 - val_loss: 0.6663 - val_accuracy: 0.6349\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5165 - accuracy: 0.7885 - val_loss: 0.6531 - val_accuracy: 0.6305\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.4893 - accuracy: 0.7966 - val_loss: 0.6357 - val_accuracy: 0.6641\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4729 - accuracy: 0.8001 - val_loss: 0.6262 - val_accuracy: 0.6658\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4569 - accuracy: 0.8030 - val_loss: 0.6222 - val_accuracy: 0.6737\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4422 - accuracy: 0.8077 - val_loss: 0.6201 - val_accuracy: 0.6817\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.4264 - accuracy: 0.8136 - val_loss: 0.6219 - val_accuracy: 0.6806\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4096 - accuracy: 0.8209 - val_loss: 0.6246 - val_accuracy: 0.6832\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.3941 - accuracy: 0.8268 - val_loss: 0.6249 - val_accuracy: 0.6877\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3764 - accuracy: 0.8359 - val_loss: 0.6254 - val_accuracy: 0.6907\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.3573 - accuracy: 0.8454 - val_loss: 0.6306 - val_accuracy: 0.6930\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.3400 - accuracy: 0.8545 - val_loss: 0.6340 - val_accuracy: 0.6895\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.3202 - accuracy: 0.8654 - val_loss: 0.6263 - val_accuracy: 0.7015\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3016 - accuracy: 0.8753 - val_loss: 0.6278 - val_accuracy: 0.7021\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.2826 - accuracy: 0.8861 - val_loss: 0.6237 - val_accuracy: 0.7123\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.2651 - accuracy: 0.8956 - val_loss: 0.6214 - val_accuracy: 0.7165\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.2488 - accuracy: 0.9043 - val_loss: 0.6334 - val_accuracy: 0.7110\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.2315 - accuracy: 0.9142 - val_loss: 0.6352 - val_accuracy: 0.7195\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.2182 - accuracy: 0.9200 - val_loss: 0.6535 - val_accuracy: 0.7147\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.2021 - accuracy: 0.9289 - val_loss: 0.6561 - val_accuracy: 0.7250\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6561 - accuracy: 0.7250\n",
      "Test Accuracy: 0.7250\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2500 dataset attempt\n",
    "\n",
    "check if increase in data set significantly improves the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puzzles shape: (2500, 10, 10)\n",
      "Solutions shape: (2500, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load(\"star_battle_dataset.npz\", allow_pickle=True)\n",
    "puzzles = dataset[\"puzzles\"]\n",
    "solutions = dataset[\"solutions\"]\n",
    "# Check the shape of the dataset\n",
    "print(\"Puzzles shape:\", puzzles.shape)\n",
    "print(\"Solutions shape:\", solutions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original puzzle count: 2500\n",
      "Augmented puzzle count: 15000\n"
     ]
    }
   ],
   "source": [
    "# augment data for larger dataset\n",
    "def augment_data(puzzles, solutions):\n",
    "    augmented_puzzles = []\n",
    "    augmented_solutions = []\n",
    "    \n",
    "    for i in range(len(puzzles)):\n",
    "        puzzle = puzzles[i]\n",
    "        solution = solutions[i]\n",
    "        \n",
    "        # original puzzle\n",
    "        augmented_puzzles.append(puzzle)\n",
    "        augmented_solutions.append(solution)\n",
    "        \n",
    "        # horizontal Flip\n",
    "        augmented_puzzles.append(np.fliplr(puzzle))\n",
    "        augmented_solutions.append(np.fliplr(solution))\n",
    "        \n",
    "        # vertical Flip\n",
    "        augmented_puzzles.append(np.flipud(puzzle))\n",
    "        augmented_solutions.append(np.flipud(solution))\n",
    "        \n",
    "        # rotate 90 degrees\n",
    "        augmented_puzzles.append(np.rot90(puzzle, k=1))\n",
    "        augmented_solutions.append(np.rot90(solution, k=1))\n",
    "        \n",
    "        # rotate 180 degrees\n",
    "        augmented_puzzles.append(np.rot90(puzzle, k=2))\n",
    "        augmented_solutions.append(np.rot90(solution, k=2))\n",
    "        \n",
    "        # rotate 270 degrees\n",
    "        augmented_puzzles.append(np.rot90(puzzle, k=3))\n",
    "        augmented_solutions.append(np.rot90(solution, k=3))\n",
    "    \n",
    "    return np.array(augmented_puzzles), np.array(augmented_solutions)\n",
    "\n",
    "# apply augmentation to your dataset\n",
    "augmented_puzzles, augmented_solutions = augment_data(puzzles, solutions)\n",
    "\n",
    "print(\"Original puzzle count:\", len(puzzles))\n",
    "print(\"Augmented puzzle count:\", len(augmented_puzzles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (2000, 10, 10), (2000, 10, 10)\n",
      "Testing data shape: (500, 10, 10), (500, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "# Reshape puzzles and solutions if necessary (ensure each puzzle is a 2D grid)\n",
    "augmented_puzzles2500 = augmented_puzzles[..., np.newaxis] # Assuming 10x10 grids, change if needed\n",
    "augmented_solutions2500 = augmented_solutions[..., np.newaxis]  # Same for solutions\n",
    "\n",
    "# Normalize puzzle values between 0 and 1 (if necessary)\n",
    "augmented_puzzles2500 = puzzles.astype(np.float32) / 9.0\n",
    "\n",
    "augmented_solutions2500 = solutions.astype(np.float32) \n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(augmented_puzzles2500, augmented_solutions2500, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replicate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,300</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m51,300\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_3 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">98,212</span> (383.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m98,212\u001b[0m (383.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">97,700</span> (381.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m97,700\u001b[0m (381.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the CNN model\n",
    "model2 = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(10, 10, 1)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(128, (1, 1), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(100, activation='sigmoid'),  # Predicts for all 100 cells\n",
    "    layers.Reshape((10, 10, 1))  # reshape back to 10x10 grid\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model2.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.6940 - loss: 0.6424 - val_accuracy: 0.6529 - val_loss: 0.6498\n",
      "Epoch 2/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7959 - loss: 0.5091 - val_accuracy: 0.6807 - val_loss: 0.6180\n",
      "Epoch 3/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7986 - loss: 0.4941 - val_accuracy: 0.7088 - val_loss: 0.5977\n",
      "Epoch 4/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7995 - loss: 0.4862 - val_accuracy: 0.7217 - val_loss: 0.5832\n",
      "Epoch 5/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8005 - loss: 0.4762 - val_accuracy: 0.7367 - val_loss: 0.5701\n",
      "Epoch 6/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8014 - loss: 0.4683 - val_accuracy: 0.7547 - val_loss: 0.5558\n",
      "Epoch 7/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8041 - loss: 0.4585 - val_accuracy: 0.7781 - val_loss: 0.5390\n",
      "Epoch 8/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8053 - loss: 0.4512 - val_accuracy: 0.7844 - val_loss: 0.5327\n",
      "Epoch 9/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8091 - loss: 0.4400 - val_accuracy: 0.7803 - val_loss: 0.5350\n",
      "Epoch 10/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8116 - loss: 0.4293 - val_accuracy: 0.7813 - val_loss: 0.5355\n",
      "Epoch 11/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8167 - loss: 0.4178 - val_accuracy: 0.7732 - val_loss: 0.5465\n",
      "Epoch 12/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8214 - loss: 0.4055 - val_accuracy: 0.7708 - val_loss: 0.5520\n",
      "Epoch 13/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8275 - loss: 0.3934 - val_accuracy: 0.7620 - val_loss: 0.5750\n",
      "Epoch 14/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8336 - loss: 0.3800 - val_accuracy: 0.7634 - val_loss: 0.5805\n",
      "Epoch 15/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8381 - loss: 0.3696 - val_accuracy: 0.7525 - val_loss: 0.5964\n",
      "Epoch 16/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8456 - loss: 0.3547 - val_accuracy: 0.7490 - val_loss: 0.6099\n",
      "Epoch 17/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8514 - loss: 0.3423 - val_accuracy: 0.7481 - val_loss: 0.6287\n",
      "Epoch 18/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8574 - loss: 0.3312 - val_accuracy: 0.7397 - val_loss: 0.6330\n",
      "Epoch 19/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8617 - loss: 0.3212 - val_accuracy: 0.7393 - val_loss: 0.6545\n",
      "Epoch 20/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8688 - loss: 0.3082 - val_accuracy: 0.7377 - val_loss: 0.6871\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7379 - loss: 0.6922 \n",
      "Test Accuracy: 0.7377\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model2.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New approach\n",
    "\n",
    "## u-net architecture with learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puzzles shape: (2500, 10, 10)\n",
      "Solutions shape: (2500, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load(\"star_battle_dataset.npz\", allow_pickle=True)\n",
    "puzzles = dataset[\"puzzles\"]\n",
    "solutions = dataset[\"solutions\"]\n",
    "# Check the shape of the dataset\n",
    "print(\"Puzzles shape:\", puzzles.shape)\n",
    "print(\"Solutions shape:\", solutions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original puzzle count: 2500\n",
      "Augmented puzzle count: 15000\n"
     ]
    }
   ],
   "source": [
    "# augment data for larger dataset\n",
    "def augment_data(puzzles, solutions):\n",
    "    augmented_puzzles = []\n",
    "    augmented_solutions = []\n",
    "    \n",
    "    for i in range(len(puzzles)):\n",
    "        puzzle = puzzles[i]\n",
    "        solution = solutions[i]\n",
    "        \n",
    "        # original puzzle\n",
    "        augmented_puzzles.append(puzzle)\n",
    "        augmented_solutions.append(solution)\n",
    "        \n",
    "        # horizontal Flip\n",
    "        augmented_puzzles.append(np.fliplr(puzzle))\n",
    "        augmented_solutions.append(np.fliplr(solution))\n",
    "        \n",
    "        # vertical Flip\n",
    "        augmented_puzzles.append(np.flipud(puzzle))\n",
    "        augmented_solutions.append(np.flipud(solution))\n",
    "        \n",
    "        # rotate 90 degrees\n",
    "        augmented_puzzles.append(np.rot90(puzzle, k=1))\n",
    "        augmented_solutions.append(np.rot90(solution, k=1))\n",
    "        \n",
    "        # rotate 180 degrees\n",
    "        augmented_puzzles.append(np.rot90(puzzle, k=2))\n",
    "        augmented_solutions.append(np.rot90(solution, k=2))\n",
    "        \n",
    "        # rotate 270 degrees\n",
    "        augmented_puzzles.append(np.rot90(puzzle, k=3))\n",
    "        augmented_solutions.append(np.rot90(solution, k=3))\n",
    "    \n",
    "    return np.array(augmented_puzzles), np.array(augmented_solutions)\n",
    "\n",
    "# apply augmentation to your dataset\n",
    "augmented_puzzles, augmented_solutions = augment_data(puzzles, solutions)\n",
    "\n",
    "print(\"Original puzzle count:\", len(puzzles))\n",
    "print(\"Augmented puzzle count:\", len(augmented_puzzles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 10, 10)\n",
      "(15000, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "print(augmented_puzzles.shape)\n",
    "print(augmented_solutions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 12000\n",
      "Val size: 1500\n",
      "Test size: 1500\n"
     ]
    }
   ],
   "source": [
    "# Reshape puzzles and solutions if necessary (ensure each puzzle is a 2D grid)\n",
    "augmented_puzzles = augmented_puzzles[..., np.newaxis] # Assuming 10x10 grids, change if needed\n",
    "augmented_solutions = augmented_solutions[..., np.newaxis]  # Same for solutions\n",
    "\n",
    "# Normalize puzzle values between 0 and 1 (if necessary)\n",
    "X = augmented_puzzles.astype(np.float32).reshape(-1, 10, 10, 1) / 9.0\n",
    "y = augmented_solutions.astype(np.float32).reshape(-1, 10, 10, 1)\n",
    "\n",
    "# split into training/validation, and testing datas\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "# split train and validation set from 90% of total data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1111, random_state=42, shuffle=True)\n",
    "\n",
    "# Check splits\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Val size: {len(X_val)}\")\n",
    "print(f\"Test size: {len(X_test)}\")\n",
    "\n",
    "# print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building unet architecture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(input_shape=(10, 10, 1), filters=64, learning_rate=0.001, dropout_rate=0.5, optimizer='adam'):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = layers.Conv2D(filters, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c2 = layers.Conv2D(filters * 2, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Decoder\n",
    "    u1 = layers.Conv2DTranspose(filters, (3, 3), strides=(2, 2), padding='same')(p1)\n",
    "    m1 = layers.Concatenate()([u1, c2])\n",
    "    c3 = layers.Conv2D(filters, (3, 3), activation='relu', padding='same')(m1)\n",
    "    \n",
    "    # Apply Dropout\n",
    "    c3 = layers.Dropout(dropout_rate)(c3)\n",
    "\n",
    "    c4 = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(c3)\n",
    "\n",
    "    # Choose optimizer\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=c4)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_unet()\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.8068 - loss: 0.4514 - val_accuracy: 0.8068 - val_loss: 0.4506\n",
      "Epoch 2/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.8090 - loss: 0.4421 - val_accuracy: 0.8083 - val_loss: 0.4438\n",
      "Epoch 3/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.8111 - loss: 0.4352 - val_accuracy: 0.8099 - val_loss: 0.4404\n",
      "Epoch 4/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.8140 - loss: 0.4268 - val_accuracy: 0.8103 - val_loss: 0.4377\n",
      "Epoch 5/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 28ms/step - accuracy: 0.8168 - loss: 0.4193 - val_accuracy: 0.8116 - val_loss: 0.4351\n",
      "Epoch 6/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.8193 - loss: 0.4121 - val_accuracy: 0.8122 - val_loss: 0.4318\n",
      "Epoch 7/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.8226 - loss: 0.4048 - val_accuracy: 0.8127 - val_loss: 0.4318\n",
      "Epoch 8/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.8248 - loss: 0.3974 - val_accuracy: 0.8105 - val_loss: 0.4380\n",
      "Epoch 9/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.8285 - loss: 0.3890 - val_accuracy: 0.8122 - val_loss: 0.4337\n",
      "Epoch 10/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.8317 - loss: 0.3819 - val_accuracy: 0.8118 - val_loss: 0.4344\n",
      "Epoch 11/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 27ms/step - accuracy: 0.8349 - loss: 0.3747 - val_accuracy: 0.8104 - val_loss: 0.4395\n",
      "Epoch 12/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.8379 - loss: 0.3678 - val_accuracy: 0.8108 - val_loss: 0.4406\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.97      0.89    120000\n",
      "         1.0       0.61      0.18      0.27     30000\n",
      "\n",
      "    accuracy                           0.81    150000\n",
      "   macro avg       0.72      0.57      0.58    150000\n",
      "weighted avg       0.78      0.81      0.77    150000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_val.flatten(), model.predict(X_val).flatten() > 0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=0.001, filters=32, batch_size=16, epochs=10, dropout=0.3, optimizer=adam\n",
      "Epoch 1/10\n",
      "844/844 [==============================] - 12s 13ms/step - loss: 0.4971 - accuracy: 0.7993 - val_loss: 0.4829 - val_accuracy: 0.8005\n",
      "Epoch 2/10\n",
      "844/844 [==============================] - 10s 12ms/step - loss: 0.4789 - accuracy: 0.8008 - val_loss: 0.4735 - val_accuracy: 0.8016\n",
      "Epoch 3/10\n",
      "844/844 [==============================] - 8s 10ms/step - loss: 0.4713 - accuracy: 0.8016 - val_loss: 0.4673 - val_accuracy: 0.8025\n",
      "Epoch 4/10\n",
      "844/844 [==============================] - 8s 10ms/step - loss: 0.4654 - accuracy: 0.8027 - val_loss: 0.4621 - val_accuracy: 0.8035\n",
      "Epoch 5/10\n",
      "844/844 [==============================] - 9s 11ms/step - loss: 0.4602 - accuracy: 0.8039 - val_loss: 0.4587 - val_accuracy: 0.8040\n",
      "Epoch 6/10\n",
      "844/844 [==============================] - 9s 11ms/step - loss: 0.4558 - accuracy: 0.8050 - val_loss: 0.4570 - val_accuracy: 0.8045\n",
      "Epoch 7/10\n",
      "844/844 [==============================] - 9s 11ms/step - loss: 0.4522 - accuracy: 0.8059 - val_loss: 0.4525 - val_accuracy: 0.8060\n",
      "Epoch 8/10\n",
      "844/844 [==============================] - 10s 11ms/step - loss: 0.4489 - accuracy: 0.8069 - val_loss: 0.4506 - val_accuracy: 0.8069\n",
      "Epoch 9/10\n",
      "844/844 [==============================] - 10s 11ms/step - loss: 0.4459 - accuracy: 0.8078 - val_loss: 0.4497 - val_accuracy: 0.8072\n",
      "Epoch 10/10\n",
      "844/844 [==============================] - 10s 12ms/step - loss: 0.4436 - accuracy: 0.8086 - val_loss: 0.4478 - val_accuracy: 0.8076\n",
      "Training with lr=0.001, filters=32, batch_size=16, epochs=10, dropout=0.3, optimizer=sgd\n",
      "Epoch 1/10\n",
      "844/844 [==============================] - 10s 12ms/step - loss: 0.6015 - accuracy: 0.7606 - val_loss: 0.5222 - val_accuracy: 0.8000\n",
      "Epoch 2/10\n",
      "844/844 [==============================] - 10s 12ms/step - loss: 0.5165 - accuracy: 0.8000 - val_loss: 0.5120 - val_accuracy: 0.8000\n",
      "Epoch 3/10\n",
      "844/844 [==============================] - 10s 12ms/step - loss: 0.5136 - accuracy: 0.8000 - val_loss: 0.5106 - val_accuracy: 0.8000\n",
      "Epoch 4/10\n",
      "844/844 [==============================] - 10s 12ms/step - loss: 0.5123 - accuracy: 0.8000 - val_loss: 0.5095 - val_accuracy: 0.8000\n",
      "Epoch 5/10\n",
      "844/844 [==============================] - 10s 12ms/step - loss: 0.5111 - accuracy: 0.8000 - val_loss: 0.5086 - val_accuracy: 0.8000\n",
      "Epoch 6/10\n",
      "842/844 [============================>.] - ETA: 0s - loss: 0.5101 - accuracy: 0.8000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, filters=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, epochs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, dropout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, optimizer=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m model \u001b[38;5;241m=\u001b[39m build_unet(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m), filters\u001b[38;5;241m=\u001b[39mf, learning_rate\u001b[38;5;241m=\u001b[39mlr, dropout_rate\u001b[38;5;241m=\u001b[39mdr, optimizer\u001b[38;5;241m=\u001b[39mopt)\n\u001b[1;32m---> 61\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Evaluate the model on validation set\u001b[39;00m\n\u001b[0;32m     64\u001b[0m score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_val, y_val, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\engine\\training.py:1729\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1715\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1716\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1717\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1727\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[0;32m   1728\u001b[0m     )\n\u001b[1;32m-> 1729\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1742\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1744\u001b[0m }\n\u001b[0;32m   1745\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\engine\\training.py:2072\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2070\u001b[0m ):\n\u001b[0;32m   2071\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 2072\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2073\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   2074\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    931\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    935\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model function with all hyperparameters included\n",
    "def build_unet(input_shape=(10, 10, 1), filters=64, learning_rate=0.001, dropout_rate=0.5, optimizer='adam'):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = layers.Conv2D(filters, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c2 = layers.Conv2D(filters * 2, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Decoder\n",
    "    u1 = layers.Conv2DTranspose(filters, (3, 3), strides=(2, 2), padding='same')(p1)\n",
    "    m1 = layers.Concatenate()([u1, c2])\n",
    "    c3 = layers.Conv2D(filters, (3, 3), activation='relu', padding='same')(m1)\n",
    "    \n",
    "    # Apply Dropout\n",
    "    c3 = layers.Dropout(dropout_rate)(c3)\n",
    "\n",
    "    c4 = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(c3)\n",
    "\n",
    "    # Choose optimizer\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=c4)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Manually loop over hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.0001],\n",
    "    'filters': [32, 64, 128],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [10, 20, 30],\n",
    "    'dropout_rate': [0.3, 0.5],\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "}\n",
    "\n",
    "# Assume X_train, y_train are your data and labels\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "# Grid Search Loop\n",
    "for lr in param_grid['learning_rate']:\n",
    "    for f in param_grid['filters']:\n",
    "        for bs in param_grid['batch_size']:\n",
    "            for ep in param_grid['epochs']:\n",
    "                for dr in param_grid['dropout_rate']:\n",
    "                    for opt in param_grid['optimizer']:\n",
    "                        print(f\"Training with lr={lr}, filters={f}, batch_size={bs}, epochs={ep}, dropout={dr}, optimizer={opt}\")\n",
    "                        \n",
    "                        model = build_unet(input_shape=(10, 10, 1), filters=f, learning_rate=lr, dropout_rate=dr, optimizer=opt)\n",
    "                        model.fit(X_train, y_train, epochs=ep, batch_size=bs, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "                        \n",
    "                        # Evaluate the model on validation set\n",
    "                        score = model.evaluate(X_val, y_val, verbose=0)\n",
    "                        if score[1] > best_score:  # score[1] is accuracy\n",
    "                            best_score = score[1]\n",
    "                            best_params = {'learning_rate': lr, 'filters': f, 'batch_size': bs, 'epochs': ep, 'dropout_rate': dr, 'optimizer': opt}\n",
    "\n",
    "print(f\"Best parameters: {best_params} with accuracy: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gridcam visualisation??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridcam visualisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
